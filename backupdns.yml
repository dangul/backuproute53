AWSTemplateFormatVersion: '2010-09-09'
Parameters:
  S3BucketName:
    Type: String
    Description: 'The name of the S3 bucket where backups will be stored. Important that this is other account than Route53'
  ScheduleExpression:
    Type: String
    Description: 'The cron expression for when the EventBridge rule should trigger the Lambda function'
    Default: 'cron(0 2 * * ? *)'
  DestinationKMS:
    Type: String
    Description: 'The arn for the KMS key on the destination account'
  ExpireLog:
    Type: String
    Description: 'Expire CloudWatch log after (days)'
    Default: '14'

Resources:
  BackupFunctionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service: 'lambda.amazonaws.com'
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: 'BackupRoute53Policy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
              - Effect: 'Allow'
                Action:
                  - 'route53:ListHostedZones'
                  - 'route53:ListResourceRecordSets'
                Resource: '*'
              - Effect: 'Allow'
                Action:
                  - 's3:PutObject'
                Resource: !Sub 'arn:aws:s3:::${S3BucketName}/*'
              - Effect: 'Allow'
                Action:
                  - 'kms:GenerateDataKey'
                Resource: !Ref DestinationKMS

  BackupFunction:
    Type: 'AWS::Lambda::Function'
    Properties:
      Handler: 'index.lambda_handler'
      Role: !GetAtt BackupFunctionRole.Arn
      Code:
        ZipFile: |
          import boto3
          import datetime
          import logging
          import os
          from concurrent.futures import ThreadPoolExecutor, as_completed
          from zoneinfo import ZoneInfo
          from datetime import datetime

          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger()

          def format_record_bind(record):
              record_type = record["Type"]
              name = record["Name"].rstrip(".")  # keep FQDN name when writing out
              ttl = record.get("TTL")  # ALIAS saknar TTL

              lines = []

              # ALIAS
              if "AliasTarget" in record:
                  target = record["AliasTarget"]["DNSName"].rstrip(".")

                  # Bind format doesn't have ALIAS; we write a comment + pseudo-line for readable backup
                  lines.append(f"; ALIAS {name} -> {target} ({record_type})")

                  # Choose a default TTL for backup text (doesn't affect Route 53)
                  alias_ttl = ttl if ttl is not None else 60
                  lines.append(f"{name}. {alias_ttl} IN {record_type} {target} ; alias")
                  return "\n".join(lines)

              #  ResourceRecords
              values = [r["Value"] for r in record.get("ResourceRecords", [])]

              # Some special records may lack values; skip empty ones
              if not values and record_type not in ("NS", "SOA"):
                  return ""

              ttl_str = str(ttl) if ttl is not None else "60"
              for value in values:
                  lines.append(f"{name}. {ttl_str} IN {record_type} {value}")

              return "\n".join(lines)

          def backup_zone(zone, date_str, bucket_name):
              route53_client = boto3.client("route53")
              s3_client = boto3.client("s3")

              raw_zone_id = zone["Id"]
              zone_id = raw_zone_id.split("/")[-1]
              zone_name = zone["Name"]

              logger.info(f"Let's backup zone: {zone_name} (ID: {zone_id})")

              try:
                  paginator = route53_client.get_paginator("list_resource_record_sets")
                  records = []
                  for page in paginator.paginate(HostedZoneId=zone_id):
                      records.extend(page["ResourceRecordSets"])

                  logger.info(f"Found {len(records)} records in zone {zone_name}.")

                  formatted = []
                  for rec in records:
                      try:
                          out = format_record_bind(rec)
                          if out:
                              formatted.append(out)
                      except Exception as e:
                          logger.exception(f"Error formatting record {rec.get('Name')} [{rec.get('Type')}] in zone {zone_name}: {e}")

                  backup_data = "\n".join(formatted) + "\n"
                  backup_filename = f"{date_str}/{zone_name.rstrip('.')}.zone"

                  s3_client.put_object(
                      Bucket=bucket_name,
                      Key=backup_filename,
                      Body=backup_data.encode("utf-8"),
                      ContentType="text/plain; charset=utf-8",
                  )
                  logger.info(f"Zone {zone_name} stored as {backup_filename}.")

              except Exception as e:
                  logger.exception(f"Backup failed for zone {zone_name} (ID: {zone_id}): {e}")
                  raise  # important: let the error bubble up so we see it in the main thread

          def lambda_handler(event, context):
              logger.info("Lambda function started.")
              route53_client = boto3.client("route53")

              # Paginate over all hosted zones
              zones = []
              paginator = route53_client.get_paginator("list_hosted_zones")
              for page in paginator.paginate():
                  zones.extend(page["HostedZones"])

              logger.info(f"Found {len(zones)} hosted zones.")

              tz_name = os.environ.get("TIMEZONE", "UTC")
              now = datetime.now(ZoneInfo(tz_name))
              date_str = now.strftime("%Y-%m-%dT%H:%MZ")

              bucket_name = os.environ["S3_BUCKET_NAME"]

              errors = 0
              with ThreadPoolExecutor(max_workers=8) as executor:
                  futures = [executor.submit(backup_zone, z, date_str, bucket_name) for z in zones]
                  for f in as_completed(futures):
                      try:
                          f.result()
                      except Exception:
                          errors += 1

              if errors:
                  msg = f"Backup complete but {errors} zone(s) failed. See logs."
                  logger.warning(msg)
                  return {"statusCode": 207, "body": msg}

              logger.info("Backup completed successfully!")
              return {"statusCode": 200, "body": "Backup completed successfully!"}

      Runtime: 'python3.13'
      Timeout: 300
      LoggingConfig:
        LogFormat: JSON
      Environment:
        Variables:
          S3_BUCKET_NAME: !Ref S3BucketName

  BackupFunctionTrigger:
    Type: 'AWS::Events::Rule'
    Properties:
      ScheduleExpression: !Ref ScheduleExpression
      Targets:
        - Arn: !GetAtt BackupFunction.Arn
          Id: 'BackupFunctionTrigger'
    DependsOn: BackupFunction

  PermissionForEventsToInvokeLambda:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !Ref BackupFunction
      Action: 'lambda:InvokeFunction'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt BackupFunctionTrigger.Arn

  BackupFunctionLogGroup:
    Type: 'AWS::Logs::LogGroup'
    Properties:
      LogGroupName: !Sub '/aws/lambda/${BackupFunction}'
      RetentionInDays: !Ref ExpireLog

  FailedBackupMetricFilter:
    Type: 'AWS::Logs::MetricFilter'
    Properties:
      LogGroupName: !Ref BackupFunctionLogGroup
      FilterPattern: '?failed'
      MetricTransformations:
        - MetricNamespace: 'Route53Backup'
          MetricName: 'FailedBackups'
          MetricValue: '1'
          DefaultValue: 0

Outputs:
  BackupFunctionArn:
    Description: 'ARN of the backup Lambda function'
    Value: !GetAtt BackupFunction.Arn